{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stop words\n",
    "stop_words=[]\n",
    "def extract_words_from_txt(file_path, encoding='latin-1'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        text = file.read()\n",
    "        words_list = text.split()\n",
    "    return words_list\n",
    "file_path = '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/ Stop words/StopWords_Names.txt'\n",
    "words_list = extract_words_from_txt(file_path)\n",
    "stop_words= stop_words+words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words=[]\n",
    "negative_words=[]\n",
    "def extract_words_from_txt(file_path, encoding='latin-1'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        text = file.read()\n",
    "        words_list = text.split()\n",
    "    return words_list\n",
    "\n",
    "# Example usage\n",
    "file_path1= '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/ MAster Dictionary/negative-words.txt'\n",
    "file_path2='/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/ MAster Dictionary/positive-words.txt'\n",
    "words_list1 = extract_words_from_txt(file_path1)\n",
    "words_list2 = extract_words_from_txt(file_path2)\n",
    "\n",
    "# Print the extracted words list\n",
    "\n",
    "\n",
    "\n",
    "# Print the extracted words list\n",
    "\n",
    "negative_words= negative_words+words_list1\n",
    "positive_words=positive_words+words_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(link):     \n",
    "        import requests\n",
    "        from bs4 import BeautifulSoup\n",
    "        import re\n",
    "\n",
    "            # Function to scrape data from URL and save in a text file\n",
    "        def scrape_and_save(url, output_file):\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                page_heading1 = soup.find('h1', class_='entry-title')\n",
    "                page_heading=page_heading1.get_text(strip=True)if page_heading1 else ''\n",
    "\n",
    "                article_content1 = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "                article_content= article_content1.get_text(strip=True) if article_content1 else ''\n",
    "                combined_data = f\"{page_heading}\\n\\n{article_content}\"\n",
    "                with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                    file.write(combined_data)\n",
    "                    print(f\"Data scraped successfully and saved to '{output_file}'.\")\n",
    "            else:\n",
    "                print(\"Failed to retrieve data from the URL.\")\n",
    "\n",
    "\n",
    "        url = \"{}\".format(link) \n",
    "        output_file = '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'  \n",
    "        scrape_and_save(url,output_file)\n",
    "\n",
    "\n",
    "                #Cleaning Data and calculating Positive and negative score using cleaned Data and creating file of positive and negative words present in cleaned data\n",
    "\n",
    "        c_positive_words = set(positive_words)\n",
    "        c_negative_words = set(negative_words)\n",
    "        c_stop_words = set(stop_words)\n",
    "        def clean_text(text):\n",
    "                    \n",
    "            words = text.split()\n",
    "                    \n",
    "            cleaned_words = [word for word in words if word.lower() not in c_stop_words]\n",
    "                    \n",
    "            cleaned_text = ' '.join(cleaned_words)\n",
    "                    \n",
    "            return cleaned_text\n",
    "\n",
    "        def count_positive_negative_words(text):\n",
    "            \n",
    "\n",
    "            positive_count = 0\n",
    "            negative_count = 0\n",
    "            clean_positive=[]\n",
    "            clean_negative=[]\n",
    "                    \n",
    "                    \n",
    "            words = text.split()\n",
    "                    \n",
    "                \n",
    "            for word in words:\n",
    "                if word in positive_words:\n",
    "                    positive_count += 1\n",
    "                    clean_positive.append(word)\n",
    "                elif word in negative_words:\n",
    "                    negative_count -= 1\n",
    "                    clean_negative.append(word)\n",
    "\n",
    "                            \n",
    "            return positive_count, negative_count*-1,clean_positive,clean_negative\n",
    "\n",
    "\n",
    "        with open(output_file, 'r', encoding='utf-8') as file:\n",
    "            extracted_data = file.read()\n",
    "\n",
    "            cleaned_data = clean_text(extracted_data)\n",
    "\n",
    "            positive_count, negative_count,clean_positive,clean_negative = count_positive_negative_words(cleaned_data)\n",
    "            for i in clean_positive:\n",
    "                with open('/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/cleaned_positive0001.txt', 'a', encoding='utf-8') as file:\n",
    "                    file.write(i+'\\n')\n",
    "                            \n",
    "            for i in clean_negative:\n",
    "                with open('/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/cleaned_negative0001.txt', 'a', encoding='utf-8') as file:\n",
    "                    file.write(i+'\\n')\n",
    "\n",
    "            #tokenizing using nltk and counting words and sentances\n",
    "\n",
    "            import nltk\n",
    "            # nltk.download('punkt')  \n",
    "            import string\n",
    "\n",
    "            def remove_punctuation_except_fullstop(text):\n",
    "                punctuation_except_fullstop = ''.join(c for c in string.punctuation if c != '.')\n",
    "                translator = str.maketrans('', '', punctuation_except_fullstop)\n",
    "                cleaned_text = text.translate(translator)\n",
    "                return cleaned_text\n",
    "\n",
    "            def count_words_and_sentences(text):\n",
    "                words = nltk.word_tokenize(text)\n",
    "                num_words = len(words)\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                num_sentences = len(sentences)\n",
    "\n",
    "                return num_words, num_sentences\n",
    "\n",
    "            cleaned_data=remove_punctuation_except_fullstop(cleaned_data)\n",
    "            word_count, sentence_count = count_words_and_sentences(cleaned_data)\n",
    "            if word_count>0:\n",
    "                average_sentence_length=round(word_count/sentence_count,0)\n",
    "            else:\n",
    "                average_sentence_length=round(word_count/1,0)\n",
    "\n",
    "            #Count Complex Words\n",
    "\n",
    "\n",
    "            def count_complex_words(text):\n",
    "                def count_syllables(word):\n",
    "                    vowels = \"aeiouy\"\n",
    "                    count = 0\n",
    "                    prev_char = None\n",
    "                    for char in word:\n",
    "                        if char.lower() in vowels and (prev_char is None or prev_char not in vowels):\n",
    "                            count += 1\n",
    "                        prev_char = char\n",
    "                    return count if count > 0 else 1  \n",
    "                words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "                complex_word_count=0\n",
    "                for word in words:\n",
    "                    if count_syllables(word)>2:\n",
    "                        complex_word_count+=1\n",
    "\n",
    "            \n",
    "\n",
    "                return complex_word_count\n",
    "\n",
    "\n",
    "            num_complex_words = count_complex_words(cleaned_data)\n",
    "\n",
    "\n",
    "            #count Syllable per word\n",
    "\n",
    "            count_syllable=0\n",
    "            def count_syllables(word):\n",
    "                vowels = \"aeiouy\"\n",
    "                count = 0\n",
    "                prev_char = ''\n",
    "                for char in word:\n",
    "                    if char.lower() in vowels and prev_char not in vowels:\n",
    "                        count += 1\n",
    "                    prev_char = char\n",
    "                return count\n",
    "            words = re.findall(r'\\b\\w+\\b', cleaned_data.lower())\n",
    "            for i in words:\n",
    "                count_syllable+=count_syllables(i)\n",
    "            if word_count>0:\n",
    "                syllable_per_word=round(count_syllable/word_count,0)\n",
    "            else:\n",
    "                syllable_per_word=round(count_syllable/1,0)\n",
    "\n",
    "\n",
    "            #Count personal Pronoun\n",
    "\n",
    "            def count_personal_pronouns(text):\n",
    "                \n",
    "                pattern = r'\\b(I|we|my|ours|us)\\b'\n",
    "            \n",
    "                regex = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "                matches = regex.findall(text)\n",
    "\n",
    "                \n",
    "                matches = [match for match in matches if match.lower() != 'us']\n",
    "\n",
    "                \n",
    "                count = len(matches)\n",
    "\n",
    "                return count\n",
    "            pronoun_count = count_personal_pronouns(extracted_data)\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate the sum of total characters in each word\n",
    "\n",
    "\n",
    "            words = cleaned_data.split()\n",
    "            total_characters = sum(len(word) for word in words)\n",
    "\n",
    "           \n",
    "            if word_count>0:\n",
    "                Polarity=((positive_count-negative_count)/((positive_count+negative_count)+0.000001))\n",
    "                Subjectivity_Score = (positive_count + negative_count)/ ((word_count) + 0.000001)\n",
    "                percentage_complex=(num_complex_words/word_count)\n",
    "                Fog_Index = 0.4 * (average_sentence_length+percentage_complex)\n",
    "                avg_word_len=total_characters/word_count\n",
    "            else:\n",
    "                Polarity=((positive_count-negative_count)/((positive_count+negative_count)+0.000001))\n",
    "                Subjectivity_Score = (positive_count + negative_count)/ ((word_count) + 0.000001)\n",
    "                percentage_complex=(num_complex_words/1)\n",
    "                Fog_Index = 0.4 * (average_sentence_length+percentage_complex)\n",
    "                avg_word_len=total_characters/1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            a=positive_count\n",
    "            b=negative_count\n",
    "            c=round(Polarity,2)\n",
    "            d=round(Subjectivity_Score,2)\n",
    "            e=average_sentence_length\n",
    "            f=round(percentage_complex*100,2),'%'\n",
    "            g=round(Fog_Index,2)\n",
    "            h=average_sentence_length\n",
    "            i=num_complex_words\n",
    "            j=word_count\n",
    "            k=syllable_per_word\n",
    "            l=pronoun_count\n",
    "            n=round(avg_word_len,0)\n",
    "            return a,b,c,d,e,f,g,h,i,j,k,l,n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Failed to retrieve data from the URL.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Failed to retrieve data from the URL.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n",
      "Data scraped successfully and saved to '/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/blackassign0008.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = \"//Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/ Output Analysi.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "df_selected = df.iloc[1:101]\n",
    "\n",
    "# Apply the function to each row and store outputs in new columns\n",
    "output_columns = ['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE','AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']  # Adjust column names as needed\n",
    "df[output_columns] = df_selected.apply(lambda row: pd.Series(output(row['URL'])), axis=1)\n",
    "\n",
    "# Save the updated data back to Excel\n",
    "output_file = \"/Users/sumlipuri/Desktop/Projects/Beautiful Soup web scrapping/ Output Analysi.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
